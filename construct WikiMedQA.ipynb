{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a032fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-04T12:03:13.694631Z",
     "start_time": "2022-03-04T12:03:10.553099Z"
    }
   },
   "outputs": [],
   "source": [
    "import pygrove\n",
    "from tqdm.auto import tqdm\n",
    "import py_stringmatching as sm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.utils import io\n",
    "import requests\n",
    "tqdm.pandas()\n",
    "from functools import cache\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "author = '' #anonymized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdcc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources= ['medwiki','wikem','wikidoc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(source):\n",
    "    df=pd.read_csv(f's3://{author}/{source}_sections.csv')\n",
    "    df['diff_diag_text']=df.diff_diag_text.map(eval)\n",
    "    df=df.fillna('')\n",
    "    df['title']=df['title'].map(lambda x: (x.split('(')[0] if not x.startswith('(') else x))\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fda1006",
   "metadata": {},
   "source": [
    "# Distractor retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf19117",
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.capture_output() as captured:\n",
    "    model = SentenceTransformer(f\"{author}/distractors_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distractors(df,key='title',n_distractors=50):\n",
    "\n",
    "    answers=list(set(df[key]))\n",
    "    Hd = model.encode([{'DOC':x} for x in answers],normalize_embeddings=True)\n",
    "    Hq = model.encode([{'QRY':x} for x in answers],normalize_embeddings=True)\n",
    "    distractors=defaultdict(list)\n",
    "    chunks= np.array_split(range(len(answers)),1000)\n",
    "    for chunk in tqdm(chunks):\n",
    "        scores=Hq[chunk]@Hd.T\n",
    "        for i,a in enumerate(chunk):\n",
    "            answer=answers[a]\n",
    "            distractors[f'AVG_{answer}'] = np.argsort(scores[i])[::-1][:n_distractors].mean()\n",
    "            for j in np.argsort(scores[i])[::-1][:n_distractors]:\n",
    "                if answer not in answers[j] and answers[j] not in answer:\n",
    "                    distractors[answer]+= [answers[j]]\n",
    "    return dict(distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interlace(l1,l2):\n",
    "    l = [a for a in fc.flatten(itertools.zip_longest(l1,l2)) if a]\n",
    "    return list(dict.fromkeys(l))\n",
    "\n",
    "def make_qa(df):\n",
    "    key='title'\n",
    "    distractors=get_distractors(df,key)\n",
    "    df['distractors']=df[key].progress_map(lambda x: distractors[x][:10])\n",
    "    df['options']=df.apply(lambda x: [x[key]] + interlace([a for a in x.diff_diag_text if a], x.distractors)\n",
    "                ,axis=1)\n",
    "    df['options_no_diffdiag']=df.apply(lambda x: [x[key]] + x.distractors,axis=1)\n",
    "    df.options=df.options.map(lambda x:[x[0]]+[a for a in x if a!=x[0]])\n",
    "    df['label']=0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b02a1",
   "metadata": {},
   "source": [
    "# Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b290ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randargmax(b,**kw):\n",
    "    b=np.array(b)\n",
    "    return np.argmax(np.random.random(b.shape) * (b==b.max()), **kw)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "\n",
    "oc = sm.similarity_measure.overlap_coefficient.OverlapCoefficient()\n",
    "sim = lambda x,y: oc.get_sim_score(*[sm.AlphabeticTokenizer().tokenize(a.lower()) for a in (x,y)])\n",
    "def diagnose(df, key='text',n=1000):\n",
    "    return df.sample(n).apply(lambda x: randargmax([sim(x[key], o) for o in x.options][:6]),axis=1).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_mask(s,to_mask,tol=0):\n",
    "    s=' '+s\n",
    "    import regex\n",
    "    to_mask=regex.escape(to_mask)\n",
    "    return regex.sub(f'[\\s(]({to_mask}){{e<={tol}}}\\)?','<unk> ', s ,flags=regex.IGNORECASE)\n",
    "    m = regex.findall(f'[\\s(]({to_mask}){{e<={tol}}}\\)?', s ,flags=regex.IGNORECASE)\n",
    "    for x in m:\n",
    "        if type(x)==tuple:\n",
    "            for e in x:\n",
    "                s=s.replace(e,' <unk>')\n",
    "        else:\n",
    "            s=s.replace(x,' <unk>')\n",
    "    return s.strip()\n",
    "\n",
    "def apply_mask(x,n_options=8, naive=False):\n",
    "    \n",
    "    s=x.text\n",
    "    tok = sm.AlphabeticTokenizer().tokenize\n",
    "    tok= WordPunctTokenizer().tokenize\n",
    "    suspects= tok(x.options[0].lower())\n",
    "    suspects=[x.options[0]]+ suspects\n",
    "            \n",
    "    for word in suspects:\n",
    "        if word in stopwords or word in string.punctuation:\n",
    "            if naive:\n",
    "                s=fuzzy_mask(s, word)\n",
    "            continue\n",
    "        p=\" \".join(x.options[1:n_options]).lower().count(word)\n",
    "        if random.random()>1/(1+p):\n",
    "            continue\n",
    "        else:\n",
    "            s=fuzzy_mask(s, word)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "def apply_mask_token(x, naive=False):\n",
    "    s=x.text\n",
    "    tok = lambda s: tz(s, add_special_tokens=False).input_ids\n",
    "    suspects= tok(x.options[0].lower())\n",
    "    l=tok(s)\n",
    "    unk_id=tz.convert_tokens_to_ids('##unk')\n",
    "    for word in suspects:\n",
    "        if naive:\n",
    "            l=[(w if w not in suspects else unk_id) for w in l]\n",
    "        continue\n",
    "        p=tok(\" \".join(x.options[1:6]).lower()).count(word)\n",
    "        if random.random()>1/(1+p):\n",
    "            continue\n",
    "        else:\n",
    "            l=[(w if w not in suspects else unk_id) for w in l]\n",
    "    return tz.decode(l)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5669b8",
   "metadata": {},
   "source": [
    "# Build and export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in ['wikem','medwiki','wikidoc']:\n",
    "    df=parse(source)\n",
    "    df=make_qa(df)\n",
    "    df['unmasked']=df['text']\n",
    "    df['text']=df.progress_apply(apply_mask,axis=1)\n",
    "    df['text_naive']=df.progress_apply(lambda x: apply_mask(x,naive=True),axis=1)\n",
    "    print(source, len(df))\n",
    "    for key in ['text','text_naive']:\n",
    "        print(key,diagnose(df, key=key))\n",
    "    df[['text','unmasked','text_naive','options','options_no_diffdiag','label']].to_csv(f's3://{author}/mc_{source}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
